# filepath: /llm_server_client/src/local_llm_inference_server_api.py
import sentencepiece as spm
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer
from transformers import GenerationConfig, TextStreamer
from flask import Flask, request, jsonify
import argparse
import json

class ModelRunner():
    def __init__(self, device):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path= setting.llm_path, trust_remote_code=True)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = AutoModelForCausalLM.from_pretrained( setting.llm_path, device_map=device).to(device)

    def run_query(self, query):
        inputs = self.tokenizer([query], return_token_type_ids=False, padding=True, return_tensors="pt").to( setting.llm_path)
        generated_ids = self.model.generate(**inputs, do_sample=True, num_beams=4, max_new_tokens=setting.max_new_tokens)
        input_length = inputs.input_ids.shape[1]
        result = self.tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0]
        return result


app = Flask(__name__)

@app.route('/llm/q', methods=['POST'])
def read_question():
    rq = request.get_json()
    question = rq.get('query', "")
    user = rq.get('user', "no user is provided")
    key = rq.get('key', "no key is provided")
    if key == "key1":
        result = llm_runner.run_query(question)
    else:
        result = "user key is unknown"
    return jsonify(message="Success", statusCode=200, query=question, answer=result), 200

if __name__ == '__main__':
    
    setting = dict
    setting.llm_path =  "" # set the path to a downloaded model or locally trained model for example "/data/llama-3.3-70b-instruct"



    setting.model_name = "openai/gpt-oss-120b"  # the default model name on HuggingFace, in case you want to download it.

    # best open source none-reaoning model (august 2025 according to https://livebench.ai/#/ ) :
    # Qwen/Qwen3-235B-A22B-Instruct-2507 
    # moonshotai/Kimi-K2-Instruct             beter score on coding and math
    # "meta-llama/Llama-3.3-70B-Instruct" best on tool use according to https://www.vellum.ai/open-llm-leaderboard?utm_source=www.vellum.ai&utm_medium=referral

    # openai/gpt-oss-120b according to OpenAI achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU
    # also Best in Reasoning (GPQA Diamond) according to https://www.vellum.ai/open-llm-leaderboard?utm_source=www.vellum.ai&utm_medium=referral

    setting.max_new_tokens = 50  # max new tokens to be generated by the model

    parser = argparse.ArgumentParser("input model name in HuggingFace or path to a local model")
    parser.add_argument("--model_name", default="", help="model name or path to the model", type=str)
    parser.add_argument("--max_new_tokens", default=500, help="max_new_tokens parameters for llms", type=int)
    parser.add_argument("--device", default="cuda:0", help="device", type=int)

    args = parser.parse_args()

    setting.llm_path = args.model_name
    setting.max_new_tokens = args.max_new_tokens

    if args.device != "cpu" and args.device != "":
        setting.device = f"cuda:{args.device}"
    if args.device == "cpu":
        setting.device = "cpu"
    if args.device == "":
        setting.device = "cuda:0"  # default device 

        

    if setting.llm_path == "":
        setting.llm_path = setting.model_name  # if you want to download the model from HuggingFace


    
    llm_runner = ModelRunner(device= setting.llm_path)

    print("Starting the server with model:", args.model_name)
    app.run(debug=False)

